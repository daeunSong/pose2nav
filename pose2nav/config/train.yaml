dataset:
  sample_file: "data/train/samples.pkl"
  train: true
  only_human_visable: true
  only_nonlinear: true
  resize: [224, 224] #[192, 192]  # decrease
  metric_waypoint_spacing: 1.0

dataloader:
  num_workers: 16  # increase #######
  batch_size: 32   # increase #######
  shuffle: True # whether to shuffle data or not
  pin_memory: True # use pageable memory or pinned memory (https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)

train_params:
  seed: 42
  epochs: 200           
  head_lr: 5e-4        
  backbone_lr: 1e-4    
  weight_decay: 1e-5
  save_every: 50
  save_best: 150
  loss: vicreg
  sample_seed: 42
  log_every: 50

model:
  d: 512
  d_goal: 64
  human_pool_temp: 1.0

  input:
    T_obs: 6
    T_pred: 12
    N_human: 6
    N_joints: 17

  image_encoder:
    name: dino
    pretrained: true

  obs_transformer:
    layers: 3
    heads: 4      # increase #######
    ff_mult: 4
    dropout: 0.1
    norm: preln        # or "postln"

  projector:
    use: true
    d_out: 2048          # 4096
    hidden: 1024       

  # downstream
  future_traj_encoder:
    hidden: 1024

loss_params:
  vicreg:
    sim_coef: 25.0
    std_coef: 25.0
    cov_coef: 5.0
    # eps: 1e-4
    
directory:
  pretext_model_dir: /home/songd/research/pose2nav/checkpoint/pretext
  downstream_model_dir: /home/songd/research/pose2nav/checkpoint/downstream 
  pretext_ckpt: ${directory.pretext_model_dir}/${logger.pretext.project}_vicreg_d${model.d}_exp2_20250903_best.pth

logger:
  pretext:
    project: socialnav_pretext
    entity: daeun-song-george-mason-university
    mode: online
    experiment_name: ${logger.pretext.project}_d${model.d}_exp2

  downstream:
    project: socialnav_downstream
    entity: daeun-song-george-mason-university
    mode: online
    experiment_name: ${logger.downstream.project}_d${model.d}_exp2