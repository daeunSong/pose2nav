dataset:
  sample_file: "data/scand/train/samples.pkl"
  train: true
  only_human_visable: true
  only_nonlinear: true
  resize: [154, 84]  # decrease
  metric_waypoint_spacing: 1.0

dataloader:
  num_workers: 16  # increase #######
  batch_size: 32   # increase #######
  shuffle: True # whether to shuffle data or not
  pin_memory: True # use pageable memory or pinned memory (https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)
  drop_last: true
  persistent_workers: true
  prefetch_factor: 4

train_params:
  seed: 42
  epochs: 200           
  save_every: 50
  save_best: 150
  loss: vicreg
  sample_seed: 42
  log_every: 50

model:
  d: 512
  human_pool_temp: 1.0

  input:
    T_obs: 6
    T_pred: 12
    N_human: 3
    N_joints: 17

  image_encoder:
    name: dino
    pretrained: true

  obs_transformer:
    layers: 3
    heads: 4      # increase #######
    ff_mult: 4
    dropout: 0.1
    norm: preln        # or "postln"

  projector:
    use: true
    d_out: 2048          # 4096
    hidden: 1024       

  downstream:
    goal_encoder:
      dims: [64,256] # MLP dimensions
      l_act: False 
      dropout: 0.2
      bias: True
  
    head:
      dims: [256, 64]
      l_act: False 
      dropout: 0.2
      bias: True

    future_traj_encoder:
      hidden: 1024

# pretext
loss_params:
  vicreg:
    sim_coef: 25.0
    std_coef: 25.0
    cov_coef: 1.0
    # eps: 1e-4

optimizer:
  adamw:  # pretext
    lr: 1e-4
    betas:
      - 0.9
      - 0.999
    eps: 1e-8
    weight_decay: 0.03
    amsgrad: False

  # sgd:  # downstream
  #   lr: 1e-3 #5e-2
  #   momentum: 0 
  #   weight_decay: 0.01 # 0 
  #   dampening: 0 
  #   nesterov: False 
  sgd:
    lr: 0.005           
    momentum: 0.9
    nesterov: True
    weight_decay: 1e-4
    dampening: 0    

directory:
  pretext_model_dir: /home/songd/research/pose2nav/checkpoint/pretext
  downstream_model_dir: /home/songd/research/pose2nav/checkpoint/downstream 
  pretext_ckpt: ${directory.pretext_model_dir}/${logger.pretext.experiment_name}_20250908_best.pth

logger:
  pretext:
    project: socialnav_pretext
    entity: daeun-song-george-mason-university
    mode: online
    experiment_name: ${logger.pretext.project}_d${model.d}_e${train_params.epochs}_exp12

  downstream:
    project: socialnav_downstream
    entity: daeun-song-george-mason-university
    mode: online
    experiment_name: ${logger.downstream.project}_d${model.d}_e${train_params.epochs}_exp12

  # exp1 : cov coeff 5.0
  # exp3 : cov_coeff 5.0 + new data
  # exp4 : new data + no augmentation + cov coeff 1.0
  # exp5 : new data + yes augmentation + cov coeff 2.0 + inplace true
  # exp6 : dino fix!!! + downstream little fix

  # exp10 : clip grad + pretxt exp 7
  # exp10 : optimizer back to adamw
  # exp11 : optimizer param fix

  ## ---
  # exp12 : only scand dataset, ped num 3, adamw